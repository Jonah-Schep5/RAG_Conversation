{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2710b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb3c895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ETL Pipeline: Hugging Face Dataset Integration\n",
      "============================================================\n",
      "No HF_TOKEN found in environment. Please login interactively:\n",
      "Successfully authenticated with Hugging Face!\n",
      "\n",
      "Loading dataset: ozmo-inc/telecom_synthetic_call_transcript_data\n",
      "Dataset loaded successfully!\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Agent_ID', 'Pilot', 'Agent_AHT', 'Call_ID', 'Category', 'Sub_Category', 'Call_Start', 'Call_End', 'Call_Transfer', 'Customer_Callback_7_Day', 'Customer_Callback_IDs_7_Day', 'CXM7', 'Transcript_JSON'],\n",
      "        num_rows: 19322\n",
      "    })\n",
      "})\n",
      "\n",
      "Dataset splits: ['train']\n",
      "\n",
      "train split:\n",
      "  - Number of rows: 19322\n",
      "  - Column names: ['Agent_ID', 'Pilot', 'Agent_AHT', 'Call_ID', 'Category', 'Sub_Category', 'Call_Start', 'Call_End', 'Call_Transfer', 'Customer_Callback_7_Day', 'Customer_Callback_IDs_7_Day', 'CXM7', 'Transcript_JSON']\n",
      "\n",
      "============================================================\n",
      "Displaying 3 sample records:\n",
      "============================================================\n",
      "\n",
      "Sample 1:\n",
      "------------------------------------------------------------\n",
      "  Agent_ID: AG-JIX-7837\n",
      "  Pilot: False\n",
      "  Agent_AHT: 474\n",
      "  Call_ID: CALL-90328077\n",
      "  Category: Wearables\n",
      "  Sub_Category: Smartwatch cellular activation\n",
      "  Call_Start: 2026-02-08T01:41:38.275376Z\n",
      "  Call_End: 2026-02-08T01:46:39.320376Z\n",
      "  Call_Transfer: True\n",
      "  Customer_Callback_7_Day: 0\n",
      "  Customer_Callback_IDs_7_Day: None\n",
      "  CXM7: None\n",
      "  Transcript_JSON: [{\"speaker\": \"agent\", \"text\": \"Welcome. I'll guide you through a single, structured setup flow to check watch compatibility, confirm line eligibility, and walk through the cellular activation path for...\n",
      "\n",
      "Sample 2:\n",
      "------------------------------------------------------------\n",
      "  Agent_ID: AG-GRO-1829\n",
      "  Pilot: False\n",
      "  Agent_AHT: 698\n",
      "  Call_ID: CALL-07251792\n",
      "  Category: Find identifiers\n",
      "  Sub_Category: IMEI/ICCID lookup\n",
      "  Call_Start: 2026-02-12T14:35:36.369742Z\n",
      "  Call_End: 2026-02-12T14:41:12.363742Z\n",
      "  Call_Transfer: False\n",
      "  Customer_Callback_7_Day: 1\n",
      "  Customer_Callback_IDs_7_Day: CB-37737391\n",
      "  CXM7: None\n",
      "  Transcript_JSON: [{\"speaker\": \"agent\", \"text\": \"Thanks for calling in. I can help you locate and verify your IMEI and ICCID for travel documentation. I'll guide you to the Settings path so you can see both identifiers...\n",
      "\n",
      "Sample 3:\n",
      "------------------------------------------------------------\n",
      "  Agent_ID: AG-FNY-4321\n",
      "  Pilot: False\n",
      "  Agent_AHT: 352\n",
      "  Call_ID: CALL-53190737\n",
      "  Category: Plans & add-ons\n",
      "  Sub_Category: International calling package\n",
      "  Call_Start: 2026-02-13T16:01:44.106721Z\n",
      "  Call_End: 2026-02-13T16:08:21.540721Z\n",
      "  Call_Transfer: True\n",
      "  Customer_Callback_7_Day: 0\n",
      "  Customer_Callback_IDs_7_Day: None\n",
      "  CXM7: None\n",
      "  Transcript_JSON: [{\"speaker\": \"agent\", \"text\": \"Hello, thanks for calling. I understand you want to enable the international calling add-on today?\", \"event\": null, \"start_offset_ms\": 0, \"start_ts\": \"2026-02-13T16:01:4...\n",
      "\n",
      "============================================================\n",
      "ETL Pipeline completed successfully!\n",
      "============================================================\n",
      "\n",
      "Dataset is now ready for further ETL processing...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ETL Pipeline Example: Hugging Face Dataset Integration\n",
    "\n",
    "This script demonstrates how to authenticate with Hugging Face and load\n",
    "the telecom synthetic call transcript dataset.\n",
    "\n",
    "Dataset: ozmo-inc/telecom_synthetic_call_transcript_data\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def authenticate_huggingface():\n",
    "    \"\"\"\n",
    "    Authenticate with Hugging Face using a token.\n",
    "    \n",
    "    The token can be provided in two ways:\n",
    "    1. Environment variable: HF_TOKEN\n",
    "    2. Interactive prompt (if no token is found)\n",
    "    \"\"\"\n",
    "    token = os.environ.get('HF_TOKEN')\n",
    "    \n",
    "    if token:\n",
    "        print(\"Authenticating with Hugging Face using token from environment variable...\")\n",
    "        login(token=token)\n",
    "    else:\n",
    "        print(\"No HF_TOKEN found in environment. Please login interactively:\")\n",
    "        login()\n",
    "    \n",
    "    print(\"Successfully authenticated with Hugging Face!\")\n",
    "\n",
    "\n",
    "def load_telecom_dataset(dataset_name=\"ozmo-inc/telecom_synthetic_call_transcript_data\"):\n",
    "    \"\"\"\n",
    "    Load the telecom synthetic call transcript dataset from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset on Hugging Face Hub\n",
    "        \n",
    "    Returns:\n",
    "        dataset: The loaded dataset object\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading dataset: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Dataset structure: {dataset}\")\n",
    "        \n",
    "        # Display basic information about the dataset\n",
    "        if hasattr(dataset, 'keys'):\n",
    "            print(f\"\\nDataset splits: {list(dataset.keys())}\")\n",
    "            \n",
    "            for split_name in dataset.keys():\n",
    "                split = dataset[split_name]\n",
    "                print(f\"\\n{split_name} split:\")\n",
    "                print(f\"  - Number of rows: {len(split)}\")\n",
    "                print(f\"  - Column names: {split.column_names}\")\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    except OSError as e:\n",
    "        # Network or file system errors\n",
    "        print(f\"Error accessing dataset: {e}\")\n",
    "        print(\"\\nPlease check your internet connection and try again.\")\n",
    "        raise\n",
    "    except PermissionError as e:\n",
    "        # Access denied\n",
    "        print(f\"Permission denied: {e}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You are authenticated with Hugging Face\")\n",
    "        print(\"2. You have been granted access to the dataset\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        # Invalid dataset name or configuration\n",
    "        print(f\"Invalid dataset configuration: {e}\")\n",
    "        print(\"\\nPlease verify the dataset name is correct: {dataset_name}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error loading dataset: {e}\")\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You are authenticated with Hugging Face\")\n",
    "        print(\"2. You have access to the dataset\")\n",
    "        print(\"3. The dataset name is correct\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def display_sample_data(dataset, num_samples=3):\n",
    "    \"\"\"\n",
    "    Display sample data from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The loaded dataset object\n",
    "        num_samples (int): Number of samples to display\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Displaying {num_samples} sample records:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get the first split (usually 'train')\n",
    "    if hasattr(dataset, 'keys'):\n",
    "        split_name = list(dataset.keys())[0]\n",
    "        data = dataset[split_name]\n",
    "    else:\n",
    "        data = dataset\n",
    "    \n",
    "    # Display samples\n",
    "    for i in range(min(num_samples, len(data))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(\"-\" * 60)\n",
    "        sample = data[i]\n",
    "        for key, value in sample.items():\n",
    "            # Truncate long text fields for display\n",
    "            if isinstance(value, str) and len(value) > 200:\n",
    "                value = value[:200] + \"...\"\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main ETL pipeline execution function.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ETL Pipeline: Hugging Face Dataset Integration\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Authenticate with Hugging Face\n",
    "    authenticate_huggingface()\n",
    "    \n",
    "    # Step 2: Load the telecom dataset\n",
    "    dataset = load_telecom_dataset()\n",
    "    \n",
    "    # Step 3: Display sample data\n",
    "    display_sample_data(dataset)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ETL Pipeline completed successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the ETL pipeline\n",
    "    dataset = main()\n",
    "    \n",
    "    # The dataset is now loaded and ready for further processing\n",
    "    # You can add your own ETL transformations here\n",
    "    print(\"\\nDataset is now ready for further ETL processing...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fdac202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = load_dataset(\"ozmo-inc/telecom_synthetic_call_transcript_data\", split=\"train\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d276e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"telecom_synthetic_call_transcript_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
